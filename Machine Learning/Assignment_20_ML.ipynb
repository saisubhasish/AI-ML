{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35867f7b",
   "metadata": {},
   "source": [
    "1. What is the underlying concept of Support Vector Machines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8fbe1",
   "metadata": {},
   "source": [
    "Ans: The underlying concept of Support Vector Machines (SVMs) is to find the optimal hyperplane that maximizes the margin between two or more classes of data. By using a technique known as the kernel trick, SVMs are able to project data into higher-dimensional space to find a separating hyperplane that maximizes the margin between two classes of data. SVMs are powerful tools for classification, regression, outlier detection, and other machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353624e",
   "metadata": {},
   "source": [
    "2. What is the concept of a support vector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc17ea",
   "metadata": {},
   "source": [
    "Ans: A support vector is a vector in a vector space that, when added to the data set, can help optimize the performance of a supervised learning algorithm. Support vectors are used in machine learning algorithms such as support vector machines (SVMs) and support vector regression (SVR). The concept of a support vector is to find a hyperplane that best divides a dataset into classes. This hyperplane is known as the decision boundary, and the support vectors are the data points that are closest to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7188799",
   "metadata": {},
   "source": [
    "3. When using SVMs, why is it necessary to scale the inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80f844",
   "metadata": {},
   "source": [
    "Ans: Scaling the inputs is necessary when using SVMs because the algorithm is sensitive to the relative size of the inputs. Without scaling, features with large values will dominate the optimization process and the model will not be able to accurately capture the underlying relationship between the features and the target. Scaling the inputs ensures that all features are given the same weight in the optimization process, allowing the model to capture more subtle patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0582f",
   "metadata": {},
   "source": [
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1a4b4",
   "metadata": {},
   "source": [
    "Ans: Yes, an SVM classifier can output a confidence score. However, it cannot output a percentage chance as this type of information is not provided by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2d966",
   "metadata": {},
   "source": [
    "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12528e5",
   "metadata": {},
   "source": [
    "Ans: It depends on the complexity of the data set and the size of the training set. Generally, the dual form of the SVM problem is more efficient and can handle datasets with a large number of features and large number of instances better than the primal form. However, if the data set is too large or too complex, it might be better to use the primal form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96732691",
   "metadata": {},
   "source": [
    "6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1b74e",
   "metadata": {},
   "source": [
    "Ans: It is better to raise gamma and lower C. Increasing gamma will make the decision boundary more complex, allowing for more complex patterns to be learned by the SVM classifier. Reducing C will increase the regularization strength, which can help to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c755f",
   "metadata": {},
   "source": [
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39615b47",
   "metadata": {},
   "source": [
    "Ans: The QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "H: The Hessian matrix - a square matrix of size nxn, where n is the number of features. The entries of H are the inner products of the feature vectors for each training example.\n",
    "\n",
    "f: A vector of size n (the number of features) with the coefficients of the linear objective function.\n",
    "\n",
    "A: A matrix of size n x m (where m is the number of constraints) with the coefficients of the linear constraints.\n",
    "\n",
    "b: A vector of size m (the number of constraints) with the right-hand side of the linear constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5014c3",
   "metadata": {},
   "source": [
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa9e28",
   "metadata": {},
   "source": [
    "Ans: Yes, it is possible to get the SVC and SGDClassifier to make a model that is similar to the LinearSVC. To do this, you will need to adjust the hyperparameters of each model, such as the regularization parameter C, the kernel type, and the margin width. You can also adjust other parameters such as the learning rate and the number of iterations. Once all of these parameters are tuned correctly, the models should produce similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fd3ff",
   "metadata": {},
   "source": [
    "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15390760",
   "metadata": {},
   "source": [
    "Ans: The level of precision you can achieve with an SVM classifier on the MNIST dataset will depend on the hyperparameters you use. Generally, SVM classifiers can achieve high levels of accuracy on the MNIST dataset; in some cases, up to 99.7% accuracy has been reported. Tuning the hyperparameters for the SVM classifier can help you achieve even better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937b677",
   "metadata": {},
   "source": [
    "10. On the California housing dataset, train an SVM regressor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8685417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c973f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa0804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2951c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed518f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124c8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884fdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6651f158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2fb00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16813306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5792291127963858\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_reg.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50bff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
